# BERT
**BERT** stands for Bidirectional Encoder Representations from Transformers. Unlike other **language representation models** that learn the semantic representation of text in one direction, **BERT** learns bidirectional representations using **Masked Language Modeling (MLM)**, a now-popular concept. During training word is masked and the objective is to predict the word based on both previous and future words. This is why it is called Bidirectional representation and indeed this was the major contribution of [paper](https://doi.org/10.48550/arXiv.1810.04805).


## TASKS
- [ ] Classifying fake news from headlines of news articles on [WELFake](https://doi.org/10.1109/TCSS.2021.3068519)

